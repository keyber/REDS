\documentclass[12pt]{article}

%% Language and font encodings
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{csquotes}
%\usepackage{fontspec}
\usepackage{xltxtra}
%\setmathfont{texgyretermes-math.otf}
\setmainfont[Ligatures=Rare]{Linux Libertine O}%

\usepackage{float}

\usepackage[framemethod=tikz]{mdframed}

\setlength{\parindent}{1em}
%\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand{\algorithmicand}{\textbf{ and }}
\algnewcommand{\algorithmicor}{\textbf{ or }}
\algnewcommand{\OR}{\algorithmicor}
\algnewcommand{\AND}{\algorithmicand}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\newcommand{\myfrac}[2]{\frac{\displaystyle {#1}}{\displaystyle {#2}}}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}

\usepackage{enumitem}

\usepackage{listings}
\lstset{language=Python} 

\usepackage{colortbl}

\usepackage[htt]{hyphenat}

\interfootnotelinepenalty=10000
\usepackage[bottom]{footmisc}

\usepackage{fancyvrb,cprotect}

%\usepackage[most]{tcolorbox}
%\definecolor{block-gray}{gray}{0.85}
%\newtcolorbox{blockquote}{colback=block-gray,grow to right by=-1mm,grow to left by=-1mm,boxrule=0pt,boxsep=0pt,breakable}

%% equations
\usepackage{amsthm}

%% theorem and proposition
\newtheorem{prop}{Proposition}
\newtheorem*{prop*}{Proposition}
\newtheorem{thm}{Théorème}

\newenvironment{myproof}[1][\proofname]{\proof[#1]\mbox{}\\*}{\endproof}

%% references shortcuts
\usepackage{suffix}
\renewcommand{\eqref}[1]{équation~\ref{#1}}
\newcommand{\algoref}[1]{algorithme~\ref{#1}}
\newcommand{\figref}[1]{figure~\ref{#1}}
\newcommand{\tabref}[1]{tableau~\ref{#1}}
\newcommand{\secref}[1]{section~\ref{#1}}
\newcommand{\probref}[1]{problème~\ref{#1}}
\newcommand{\propref}[1]{proposition~\ref{#1}}
\newcommand{\theoremref}[1]{théorème~\ref{#1}}
\newcommand{\chapref}[1]{chapitre~\ref{#1}}
\newcommand{\appref}[1]{annexe~\ref{#1}}
\WithSuffix\newcommand\algoref*[1]{algorithme~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\figref*[1]{figure~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\eqref*[1]{équation~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\tabref*[1]{tableau~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\secref*[1]{section~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\probref*[1]{problème~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\propref*[1]{proposition~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\chapref*[1]{chapitre~\ref{#1} p.~\pageref{#1}}

\usepackage[backend=biber,style=authoryear-comp,uniquename=init,firstinits=true,
            %% "et al" pour > deux auteurs, & pour exactement 2
            uniquelist=false,maxcitenames=2,mincitenames=1,maxbibnames=99,
            isbn=false,url=false,doi=false
]{biblatex}

\DefineBibliographyExtras{french}{\renewcommand*\mkbibnamefamily[1]{#1}}
\renewcommand{\cite}{\parencite}
\renewcommand*{\nameyeardelim}{\addcomma \addnbspace}
% \renewcommand*{\multinamedelim}{\space} % fait le contraire de ce qu'on veut
\renewcommand*{\revsdnamedelim}{}
\renewcommand*\finalnamedelim{ \& }

\DefineBibliographyExtras{french}{\restorecommand\mkbibnamelast}

\DeclareNameAlias{default}{last-first}
\DeclareNameAlias{sortname}{last-first}

% Supprimer les guillemets dans les titres
\DeclareFieldFormat[article,incollection,unpublished,inproceedings]{title}{#1}

% Espaces insÃ©cables dans les citations et la bibliographie (noms de
% confÃ©rences) ?

\usepackage{xpatch}
\usepackage{xstring}
%\xpatchbibmacro{series+number}{\addspace}{\addnbspace}{}{}

\renewbibmacro*{series+number}{%
  \setunit*{\addnbspace}%
  \printfield{series}%
  \printfield{number}%
  \newunit}

\AtBeginDocument{ %Bizarrerie unicode-math
  \DeclareMathOperator{\mmin}{\mathrm{min}}
  \DeclareMathOperator{\mmax}{\mathrm{max}}
  \DeclareMathOperator{\eexp}{\mathrm{exp}}
  \DeclareMathOperator{\argmin}{\text{argmin}}
  \newcommand{\gargmin}[2]{\argmin_{#1}\left\{{#2}\right\} }
  \DeclareMathOperator{\prox}{\mathrm{prox}}
  \newcommand{\proxg}[3]{\prox_{\frac{#1}{#2}{#3}}}
}

\makeatletter
\newcommand*{\transpose}{%
  {\mathpalette\@transpose{}}%
}
\newcommand*{\@transpose}[2]{%
  % #1: math style
  % #2: unused
  \raisebox{0.2em}{$\m@th#1\intercal$}%
}
\makeatother

\newcommand{\tr}[1]{ {#1}^{\! \transpose}}

\xpatchbibmacro{textcite}{\addcoma}{}{}{}

\addbibresource{references.bib}

\usepackage{array,multirow}
\addto\captionsfrench{\def\tablename{TABLEAU}}


\begin{document}

\begin{titlepage}
  \begin{center}

      \makebox[0.5\textwidth][c]{%
        \includegraphics[width=0.33\textwidth]{images/sorbonne.png}%
    }%

    \vspace{4cm}
    % Title
    \HRule \\[0.4cm]
    { \huge \bfseries REDS - Projet Boson de Higgs\\[0.4cm] }

      \textsc{\LARGE Rapport}\\[0.4cm]

    \HRule \\[0.4cm]

    % Author and supervisor
    \begin{minipage}{0.4\textwidth}
      \begin{flushleft} \large
        Kim-Anh Laura \textsc{Nguyen}\\
        Keyvan \textsc{Beroukhim} \\
        Master 2 DAC \\
        Promo 2019-2020 \\
      \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
      \begin{flushright} \large
          \emph{Enseignant :} Olivier \textsc{Schwander} \\
      \end{flushright}
    \end{minipage}

      \vspace{2cm}


  \end{center}
  %\end{sffamily}
\end{titlepage}
%\maketitle

\newpage

\section{Introduction}

Nous souhaitons détecter la présence du boson de Higgs dans des données simulées
dans le but de reproduire le comportement de l'expérience ATLAS. Il s'agit d'un
problème de détection d'évènement, ou classification binaire, dans lequel les
deux classes sont :

\begin{itemize}
    \item \emph{background}
    \item \emph{tau tau decay of a Higgs boson}
\end{itemize}

Les données sont issues du projet Kaggle \emph{ATLAS Higgs Boson Machine Learning Challenge
2014}. 

\section{Analyse préliminaire des données}

La base de données est constituée de 818238 évènements simulés. Chaque évènement
est défini par 30 attributs numériques et un label à prédire. \\

Les données sont composées à \textbf{34\% de labels positifs} (les
\emph{signaux}) et à 66\% de labels négatifs (le \emph{background}). Les classes
ne sont pas suffisamment déséquilibrées pour gêner l'entraînement. Par ailleurs,
la métrique propre à cette tâche, nommée \emph{AMS}, nous est imposée. Le choix
habituel de la mesure d'évaluation à utiliser pour prendre en compte ce
déséquilibre ne se pose donc pas. \\

\subsection{Distribution des variables}

La \figref{img:hist-sample} contient les histogrammes de répartition des valeurs
de quelques variables, en omettant les valeurs manquantes. Nous constatons que
\textbf{toutes les variables ne suivent pas le même type de distribution}. Or, les
algorithmes d'apprentissage se comportent généralement mieux avec une
distribution des données équilibrée. Dans ce dataset, de nombreuses variables
ont une distribution exponentielle décroissante (e.g \texttt{DER\_MASS\_VIS} sur
la \figref{img:hist-sample}).

\begin{figure}[H]
    \center
    \includegraphics[width=0.5\textwidth]{images/histograms_sample.png}
    \caption{Histogrammes de répartition des valeurs de quelques variables}
    \label{img:hist-sample}
\end{figure}

Afin d’obtenir des \textbf{distributions normales} et de \textbf{stabiliser la
variance}, nous effectuons une \textbf{log-transformation} de manière
indépendante sur les colonnes. Nous commençons par ajouter une constante à
chaque colonne à transformer pour que la valeur minimale de sur cette colonne
soit 1 (comme le log ne prend que des valeurs strictement positives). Nous
appliquons ensuite la fonction log sur les colonnes. Avec ce dataset et en
utilisant cette méthode, nous obtenons toujours des distribution normales.
Finalement nous \textbf{centrons et réduisons} chaque variable de manière
indépendante car cela facilite généralement l’apprentissage des modèles. \\

\begin{figure}[H]
	\centering
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{images/histograms_before_log.png}
    \caption{Avant log-transformation}
    \end{subfigure}

    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{images/histograms_after_log.png}
    \caption{Après log-transformation}
    \end{subfigure}

    \caption{Histogrammes de répartition des valeurs de quelques variables avant
    et après log-transformation}
    \label{fig:hist-log}
\end{figure}

\subsection{Valeurs manquantes}

Le dataset contient \textbf{21\% de valeurs manquantes} (indiquées par la valeur
-999).  D’une manière générale, les modèles d’apprentissage statistique ne
gèrent pas automatiquement les valeurs manquantes. Il faut donc traiter ces
données avant de les présenter à nos modèles. Nous considérons trois façons de
pallier à ce problème.

\subsubsection{Omission}

En supprimant les évènements dont au moins un attribut n'est pas valide, nous
retrouvons avec seulement 223574 exemples d’apprentissage : \textbf{restreindre
le dataset aux données complètes fait perdre 75\% des évènements}. Par ailleurs,
la base restreinte contient 47\% de labels positifs, soit 13\% de plus que le
dataset original. Cela met en évidence le fait que les données ne sont pas
manquantes de manière indépendante : l’absence des données est liée à leur
valeur (nous sommes dans un contexte \emph{Missing Not At Random}). Un modèle
entraîné sur le dataset restreint en faisant l’hypothèse "i.i.d." serait donc
biaisé.

\subsubsection{Omission d'attributs}

Une autre méthode consiste à \textbf{supprimer les features dont le pourcentage
de valeurs manquantes dépasse un certain seuil}. On \textbf{retire ensuite les
évènements à valeurs manquantes}. Le dataset final ne contient donc plus aucune
donnée manquante. En fixant le seuil à 40\%, notre jeu de données passe de
818238 à 693636 échantillons, soit 85\% des données initiales, et de 35 à 20
attributs.


\subsubsection{Conservation}

Les \textbf{\emph{Random Forest}} font partie des algorithmes permettant de
\textbf{gérer les données manquantes}. En effet, les arbres de décision peuvent
\textbf{reconnaître une donnée manquante par un simple test de valeur}.

\subsubsection{Affectation (\emph{Imputing})}

Une autre méthode consiste à \textbf{compléter les données manquantes} de la
manière la plus "intelligente" possible. Une fois les données complétées, cela
permet d’\textbf{utiliser tous les modèles de classification à notre
disposition}. 

Nous nous servons de l’algorithme \texttt{IterativeImputing} de
scikit-learn, qui complète de manière itérative les données manquantes en
utilisant des modèles de régression prédisant la valeur d’une variable à partir
de la valeur des autres variables. \\

En pratique et comme nous souhaitons conserver le maximum d'information,
\textbf{nous ne supprimerons ni d'évènements ni d'attributs à valeurs
manquantes}. Nous testerons, d'un côté, des \textbf{algorithmes permettant de
gérer les données manquantes} et, de l'autre, la méthode de \textbf{complétion
des données}. Pour cette dernière, nous appliquons d'abord les
\textbf{log-transformations} puis nous réalisons l’\textbf{imputing} avant de
finir par \textbf{scaler} les données. Ce choix se justifie premièrement par
l’intuition que l’imputing fonctionnera mieux sur des données log-transformées
et, deuxièmement, par le fait qu’estimer la moyenne et la variance en ignorant
les données manquantes serait biaisé.

\subsection{Sélection de features}

Les données sont représentées par \textbf{30 variables explicatives}. Ce nombre
n’étant \textbf{pas très élevé} (par exemple comparé au nombre de pixels dans
une image), il n’y a donc \textbf{pas de grand risque de sur-apprentissage} pour
les modèles. Ainsi, la sélection de caractéristique ne semble pas nécessaire.
Cependant, comme le montre la \figref{img:corr-mat}, certaines variables sont
\textbf{fortement corrélées} (e.g. \texttt{DER\_mass\_MMC} et
\texttt{DER\_mass\_transverse\_met\_lep}). Or, des attributs corrélés peuvent
diminuer la performance de certains algorithmes (en contribuant au
sur-apprentissage par exemple).

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Afin de réduire le nombre de variables explicatives, nous comparerons donc les
performances avec ou sans l’utilisation d’une \textbf{\emph{Analyse en
Composantes Principales} (PCA)}. Pour choisir le nombre de composantes
principales, nous traçons le pourcentage de variance expliquée en fonction du
nombre de composantes (\figref{img:explained-var}). Avec 20 composantes, nous
conservons $98.2\%$ de la variance totale.
\end{mdframed}

\begin{figure}[H]
    \center
    \includegraphics[width=0.7\textwidth]{images/correlation_matrix.png}
    \caption{Matrice de corrélation entre chaque attribut}
    \label{img:corr-mat}
\end{figure}

\begin{figure}[H]
    \center
    \includegraphics[width=0.75\textwidth]{images/explained_variance.png}
    \caption{Pourcentage de variance expliquée en fonction du nombre de
    composantes}
    \label{img:explained-var}
\end{figure}

\section{Classification des évènements}

\subsection{Modèles utilisés}

Nous considérons plusieurs modèles d'apprentissage pour classifier les
évènements :

\begin{itemize}
    \item SVM
    \item méthodes ensemblistes : Bagging de Perceptron, Random Forest, AdaBoost
\end{itemize}

\subsubsection{Support Vector Machines}

Les \emph{Support Vector Machines} (SVMs) nous fournissent une
\textbf{\emph{baseline}}. Généralisation des classifieurs linéaires, ils
nécessitent un \textbf{faible nombre d'hyperpararamètres}, ont des
\textbf{garanties théoriques} et donnent de \textbf{bons résultats} en pratique.

\subsubsection{Méthodes d'ensemble}

Afin de \textbf{réduire de réduire le biais et la variance} de nos modèles et
ainsi améliorer les performances, nous utilisons des \textbf{méthodes
d'ensemble}. Ces algorithmes consistent à \textbf{combiner plusieurs classifieurs
faibles} pour obtenir un modèle \textbf{plus stable, plus robuste et ayant une meilleure
capacité de généralisation}. \\

\begin{itemize}
    \item Bootstrap AGGregatING (Bagging) : 
        
        Le \emph{Bootstrap Aggregating} est une méthode pour \textbf{réduire la
        variance par moyennage}.  Plusieurs ensembles d'apprentissage sont
        simulés par \textbf{\emph{bootstrap} (tirage avec remise)} puis
        \textbf{sur chaque sous ensemble, on entraîne un classifieur faible}.
        Dans notre contexte de classification binaire, la prédiction du modèle
        est définie par le \textbf{vote majoritaire des modèles simples
        (\emph{aggregating})} .

Comme chaque classifieur faible est entraîné sur des données légèrement
différentes, le modèle d'ensemble est capable de \textbf{capturer de petites
variations dans les données} et donc de mieux \textbf{généraliser}. 

Nous choisissons, uniquement pour le Bagging, le \textbf{Perceptron} comme
classifieur faible. Il s'agit d'un \textbf{modèle instable}, ce qui permet
d'obtenir un \textbf{ensemble de classifieurs suffisamment différents}. \\

\item Random Forest : 
    
    Avec la technique du Bagging, même si chaque classifieur est appris sur un
        jeu de données légèrement différent, tous les modèles se servent des
        mêmes attributs pour partitionner les données, les arbres générés
        risquent donc d'être \textbf{trop similaires}. Pour pallier à ce
        problème, les arbres des \textbf{forêts aléatoires (\emph{random
        forests})}, également appris sur des bootstrap de l'ensemble original,
        \textbf{sélectionnent pour chacun de leurs noeuds un échantillon
        aléatoire de features}, ce qui \textbf{force les modèles à être
        différents} et empêche le modèle global de se concentrer sur des
        attributs particuliers. \\

    \item AdaBoost : 
        
        Avec la technique de Bagging, les classifieurs faibles sont appris
        indépendamment les uns des autres. Or, si ce classifieur est, à lui
        seul, trop faible, \textbf{le Bagging ne permettra pas d'obtenir un
        meilleur biais}. Le \textbf{\emph{Boosting}} permet de traiter ce
        problème par \textbf{apprentissage successif de classifieurs faibles} :
        en donnant plus de poids aux exemples mal classés par les modèles
        précédents, \textbf{chaque nouveau classifieur se focalise sur les
        parties de l'espace mal prédites}.  Contrairement au Bagging, la
        contribution d'un classifieur à la prédiction du modèle d'ensemble est
        déterminée par sa performance.

En revanche, si le classifieur faible est trop complexe, le modèle de Boosting
        \textbf{risque de sur-apprendre}, auquel cas la technique de Bagging est
        plus adaptée.

\end{itemize}

\subsection{Protocole d'expérimentation} 

Tout d'abord, nous commençons par \textbf{séparer le jeu de données en train et
en test}.  Avant d'entraîner nos modèles, nous apprenons les \textbf{paramètres
de prétraitement des données} sur l'ensemble d'apprentissage, et nous appliquons
les prétraitements sur l'ensemble des données. 

Les prétraitements et apprentissages des paramètres sont effectués dans l'ordre
décrit précédemment: \textbf{log-transformation} (valeurs minimales des colonnes
apprises), \textbf{imputing} (paramètres de tous les régresseurs appris) et
\textbf{scaling} (moyennes et écarts-types des colonnes appris).
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20] De plus, nous
    comparerons les résultats obtenus avec ces prétraitements à d'autres obtenus
    en appliquant en plus une \textbf{PCA} (à 20 composantes principales) aux
    données.
\end{mdframed}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20] En parallèle, pour
    pouvoir comparer l'approche de complétion à celle de conservation des
    données, nous effectuons un scaling des données d'origine et remplaçons les
    valeurs manquantes par une valeur aberrante. Nous ne rajoutons pas de PCA à
    ces prétraitements. Par la suite, les expériences seront dédoublées : elles
    seront menées d'un côté sur les données complétées, de l'autre sur les
    données conservées.  \end{mdframed}

L'ensemble des paramètres est appris sur les données d'apprentissage et les
prétraitements sont appliqués sur la totalité des données. \\

Pour chaque algorithme, nous procédons par \textbf{\emph{grid search} pour
trouver les hyperparamètres optimaux}. Pour chaque combinaison
d'hyperparamètres, la performance du modèle produit est estimée par
\textbf{\emph{cross-validation}} sur l'ensemble d'apprentissage. \\

Pour chaque modèle, nous récupérons les hyperparamètres ayant produit le
meilleur résultat et \textbf{calculons le score final par cross-validation sur les
données de test}. \\

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20] Nous répétons les
processus de grid-search et d'évaluation avec les données réduites à 20
composantes principales.  \\ \end{mdframed}

Par souci de temps de calcul, les expériences de recherche de paramètres
optimaux et d'évaluation des modèles ne sont pas menées sur le dataset entier.
Pour les \textbf{métriques usuelles}, \textbf{le score obtenu par un modèle est
indépendant de la taille du jeu de test}, mais \textbf{ce n'est pas le cas pour
la métrique AMS}.  Afin d'obtenir des scores en AMS comparables, nous fixons la
taille des jeux de tests à 1000 éléments. \\

\textbf{Les scores obtenus en AMS étant difficilement interprétables}, nous
utilisons aussi comme métrique le \textbf{taux de bonne classification
(\emph{accuracy})}. Pour chaque métrique, les scores présentés par la suite sont
obtenus en optimisant les modèles pour cette métrique.

\subsection{Résultats et analyse}

Pour le SVM, nous cherchons la valeur optimale de la pénalité $C$ du terme
d'erreur ainsi que le meilleur noyau à utiliser. 

$C$ correspond à la constante de \emph{soft margin}. Cet hyperparamètre influe
sur le compromis entre l'erreur en apprentissage et la maximisation de la marge.
Plus sa valeur est petite, plus le modèle sera tolérant envers les exemples mal
classés ou dans la marge, ce qui correspond à élargir cette dernière. Nous
faisons varier $C$ parmi 100 valeurs entre 0.1 et 10 équitablement réparties sur
une échelle logarithmique.

De plus, nous cherchons le noyau le plus adapté : \emph{Radial Basis Function}
(rbf) ou un noyau polynomial (poly). Cet hyperparamètre définit le type
d'hyperplan utilisé pour séparer les données.  \\

La \figref{tab:svm-parameters} contient les hyperpamètres optimisant le SVM pour
chaque métrique, avec ou sans PCA. Nous constatons que le noyau rbf permet
d'obtenir les meilleures performances dans chaque cas. Les valeurs de $C$
maximisant les scores en AMS sont significativement plus élevées que celles
maximisant l'accuracy, ce qui souligne la différence entre ces deux problèmes
d'optimisation.\\

\todo[inline, color=yellow!40]{Explication des hyperparamètres à mettre dans
protocole ou analyse ?}

\begin{table}[H]
    \centering
    \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{c|*{3}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{Accuracy} &
            \multicolumn{2}{c}{AMS}
            \\ \cline{2-3} \cline{4-5}
            & sans PCA & avec PCA & sans PCA & avec PCA\\
            \hline
            C & 0.98 & 0.74 & 2.85 & 6.28 \\
            Noyau & rbf & rbf & rbf & rbf \\
        \end{tabular}}
    \caption{Paramètres optimaux pour le SVM}
    \label{tab:svm-parameters}
\end{table}

Pour les méthodes d'ensemble, nous faisons varier le nombre d'estimateurs (50,
500, 1000 ou 2000), qui correspond à l'hyperparamètre influant le plus sur la
précision de la prédiction. De plus, nous cherchons la profondeur maximale
optimale des Random Forests.  \\

Les hyperparamètres optimaux obtenus pour chaque méthode d'ensemble, chaque
métrique et en considérant l'intégralité des variables figurent dans le
\tabref{tab:ensemble-parameters}. Nous ne remarquons pas de phénomènes
particuliers : ces hyperparamètres n'évoluent pas de façon monotone lorsque nous
passons d'une méthode/métrique à une autre.
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20] Dans le cas des
    données réduites à 20 composantes, les hyperparamètres optimaux obtenus
    sont, pour la plupart, différents et, une fois de plus, nous ne constatons
pas d'évolution particulière. Nous décidons donc de ne pas les afficher. \\
\end{mdframed}

\begin{table}[H]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|*{7}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{Bagging} &
            \multicolumn{2}{c|}{RF} &
            \multicolumn{2}{c|}{RFNan} &
            \multicolumn{2}{c}{AdaBoost}
            \\ \cline{2-3} \cline{4-5} \cline{6-7} \cline{8-9}
            & Accuracy & AMS & Accuracy & AMS & Accuracy & AMS & Accuracy & AMS\\
            \hline
            Nombre d'estimateurs & 500 & 500 & 500 & 500 & 1000 & 2000 & 1000 & 50 \\
            Profondeur max & \cellcolor{gray!40} & \cellcolor{gray!40} & $\infty$ & 20 &
            50 & $\infty$ & \cellcolor{gray!40} & \cellcolor{gray!40} \\
        \end{tabular}}
    \caption{Paramètres optimaux pour les méthodes d'ensemble (sans PCA)}
    \label{tab:ensemble-parameters}
\end{table}

%\begin{table}[H]
%    \resizebox{\textwidth}{!}{
%        \begin{tabular}{c|*{5}{c|}c}
%            \multirow{2}{*}{} &
%            \multicolumn{2}{c|}{Bagging} &
%            \multicolumn{2}{c|}{RF} &
%            \multicolumn{2}{c}{AdaBoost}
%            \\ \cline{2-3} \cline{4-5} \cline{6-7} 
%            & Accuracy & AMS & Accuracy & AMS & Accuracy & AMS\\
%            \hline
%            Nombre d'estimateurs & 2000 & 500 & 500 & 2000 & 500 & 50 \\
%            Profondeur max & \cellcolor{gray!40} & \cellcolor{gray!40} & 50 & 50 & \cellcolor{gray!40} & \cellcolor{gray!40} \\
%        \end{tabular}}
%    \caption{Paramètres optimaux pour les méthodes d'ensemble (avec PCA)}
%    \label{tab:ensemble-parameters-pca}
%\end{table}

Sur la \figref{img:ams-C} est tracé l'AMS moyen obtenu en apprentissage en
fonction de la valeur de $C$ (sans PCA). Nous remarquons que le score augmente
jusqu'à $C \approx 4$ puis stagne.

\begin{figure}[H]
    \center
    \includegraphics[width=0.7\textwidth]{images/ams.png}
    \caption{AMS moyen obtenu avec le SVM en fonction de la vaeur de $C$}
    \label{img:ams-C}
\end{figure}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Les tableaux \ref{tab:ensemble-results} et \ref{tab:ensemble-results-pca}
contiennent les scores en accuracy et AMS obtenus avec chaque méthode et,
respectivement, sans et avec PCA. Pour chaque métrique et chaque ensemble sur
lequel la performance est calculée, le meilleur score est surligné en vert.
\end{mdframed}

\begin{table}[H]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|*{3}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{\% Accuracy} &
            \multicolumn{2}{c}{AMS ($10^{-2}$)}\\ \cline{2-3} \cline{4-5}
            & Apprentissage & Test & Apprentissage & Test \\
            \hline
            SVM & $75.90 \pm 1.67$ & $75.00 \pm 1.16$ & $ 4.35 \pm 0.39 $ &
            $3.19 \pm 0.52$ \\
            Bagging & $71.90 \pm 1.29$ & $72.60 \pm 0.98$ & $3.16 \pm 0.60$ &
            $2.23 \pm 0.24$\\
            RF & \cellcolor{green!25}$80.70 \pm 1.24$ & $78.70 \pm 1.59$ & $5.07
            \pm 0.96$ & $3.98 \pm 0.16$\\
            RFNan & $80.20 \pm 0.44$ & \cellcolor{green!25}$81.30 \pm 0.62 $ &
            \cellcolor{green!25}$5.15 \pm 1.12$ & \cellcolor{green!25}$4.00 \pm
            0.15$\\
            AdaBoost & $73.30 \pm 0.76$ & $73.30 \pm 1.49$ & $3.92 \pm 0.55$ &
            $3.42 \pm 0.16$\\
        \end{tabular}}
    \caption{Performances obtenues avec chaque méthode (sans PCA)}
    \label{tab:ensemble-results}
\end{table}

\todo[inline, color=red!40]{À commenter \\}

\begin{table}[H]
    \center
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|*{3}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{\% Accuracy} &
            \multicolumn{2}{c}{AMS ($10^{-2}$)}\\ \cline{2-3} \cline{4-5}
            & Apprentissage & Test & Apprentissage & Test \\
            \hline
            SVM & \cellcolor{green!25}$76.60 \pm 0.52$ & $
            \cellcolor{green!25}76.50 \pm 0.93 $ & \cellcolor{green!25}$4.13 \pm
            0.47$ & \cellcolor{green!25}$2.80 \pm 0.39 $\\
            Bagging & $70.80 \pm 0.57$ & $70.00 \pm 0.51$ & $2.01 \pm 0.32$ &
            $1.49 \pm 0.26$  \\ 
            RF & $76.30 \pm 0.49$ & $75.30 \pm 1.44 $ & $3.72 \pm 0.62$ & $2.66
            \pm 0.07$\\
            AdaBoost & $67.70 \pm 0.42$ & $67.30 \pm  0.43$ & $3.12 \pm 0.32$ &
            $2.50 \pm 0.33$ \\
        \end{tabular}}
    \caption{Performances obtenues avec chaque méthode (avec PCA)}
    \label{tab:ensemble-results-pca}
\end{table}

\todo[inline, color=red!40]{À commenter \\}

\begin{figure}[H]
	\centering
    \begin{subfigure}[c]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/RF100.png}
    \end{subfigure}
    \begin{subfigure}[c]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/RF1000.png}
    \end{subfigure}
\end{figure}

\todo[inline, color=yellow!40]{Remplacer par les nouveaux plots \\}

\todo[inline, color=red!40]{À commenter \\}

\begin{figure}[H]
    \center
    \includegraphics[width=\textwidth]{images/ROC_curves.png}
    \caption{Courbes ROC pour chaque méthode}
    \label{img:roc-curves}
\end{figure}

\todo[inline, color=red!40]{À commenter \\}


\newpage
\printbibliography
\end{document}


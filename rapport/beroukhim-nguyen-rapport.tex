\documentclass[12pt]{article}

%% Language and font encodings
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{csquotes}
%\usepackage{fontspec}
\usepackage{xltxtra}
%\setmathfont{texgyretermes-math.otf}
\setmainfont[Ligatures=Rare]{Linux Libertine O}%

\usepackage{float}

\usepackage{bm}

\usepackage[framemethod=tikz]{mdframed}

\setlength{\parindent}{1em}
%\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand{\algorithmicand}{\textbf{ and }}
\algnewcommand{\algorithmicor}{\textbf{ or }}
\algnewcommand{\OR}{\algorithmicor}
\algnewcommand{\AND}{\algorithmicand}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\newcommand{\myfrac}[2]{\frac{\displaystyle {#1}}{\displaystyle {#2}}}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}

\usepackage{enumitem}

\usepackage{listings}
\lstset{language=Python} 

\usepackage{colortbl}

\usepackage[htt]{hyphenat}

\interfootnotelinepenalty=10000
\usepackage[bottom]{footmisc}

\usepackage{fancyvrb,cprotect}

%\usepackage[most]{tcolorbox}
%\definecolor{block-gray}{gray}{0.85}
%\newtcolorbox{blockquote}{colback=block-gray,grow to right by=-1mm,grow to left by=-1mm,boxrule=0pt,boxsep=0pt,breakable}

%% equations
\usepackage{amsthm}

%% theorem and proposition
\newtheorem{prop}{Proposition}
\newtheorem*{prop*}{Proposition}
\newtheorem{thm}{Théorème}

\newenvironment{myproof}[1][\proofname]{\proof[#1]\mbox{}\\*}{\endproof}

%% references shortcuts
\usepackage{suffix}
\renewcommand{\eqref}[1]{équation~\ref{#1}}
\newcommand{\algoref}[1]{algorithme~\ref{#1}}
\newcommand{\figref}[1]{figure~\ref{#1}}
\newcommand{\tabref}[1]{tableau~\ref{#1}}
\newcommand{\secref}[1]{section~\ref{#1}}
\newcommand{\probref}[1]{problème~\ref{#1}}
\newcommand{\propref}[1]{proposition~\ref{#1}}
\newcommand{\theoremref}[1]{théorème~\ref{#1}}
\newcommand{\chapref}[1]{chapitre~\ref{#1}}
\newcommand{\appref}[1]{annexe~\ref{#1}}
\WithSuffix\newcommand\algoref*[1]{algorithme~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\figref*[1]{figure~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\eqref*[1]{équation~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\tabref*[1]{tableau~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\secref*[1]{section~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\probref*[1]{problème~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\propref*[1]{proposition~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\chapref*[1]{chapitre~\ref{#1} p.~\pageref{#1}}

\usepackage[backend=biber,style=authoryear-comp,uniquename=init,firstinits=true,
            %% "et al" pour > deux auteurs, & pour exactement 2
            uniquelist=false,maxcitenames=2,mincitenames=1,maxbibnames=99,
            isbn=false,url=false,doi=false
]{biblatex}

\DefineBibliographyExtras{french}{\renewcommand*\mkbibnamefamily[1]{#1}}
\renewcommand{\cite}{\parencite}
\renewcommand*{\nameyeardelim}{\addcomma \addnbspace}
% \renewcommand*{\multinamedelim}{\space} % fait le contraire de ce qu'on veut
\renewcommand*{\revsdnamedelim}{}
\renewcommand*\finalnamedelim{ \& }

\DefineBibliographyExtras{french}{\restorecommand\mkbibnamelast}

\DeclareNameAlias{default}{last-first}
\DeclareNameAlias{sortname}{last-first}

% Supprimer les guillemets dans les titres
\DeclareFieldFormat[article,incollection,unpublished,inproceedings]{title}{#1}

% Espaces insÃ©cables dans les citations et la bibliographie (noms de
% confÃ©rences) ?

\usepackage{xpatch}
\usepackage{xstring}
%\xpatchbibmacro{series+number}{\addspace}{\addnbspace}{}{}

\renewbibmacro*{series+number}{%
  \setunit*{\addnbspace}%
  \printfield{series}%
  \printfield{number}%
  \newunit}

\AtBeginDocument{ %Bizarrerie unicode-math
  \DeclareMathOperator{\mmin}{\mathrm{min}}
  \DeclareMathOperator{\mmax}{\mathrm{max}}
  \DeclareMathOperator{\eexp}{\mathrm{exp}}
  \DeclareMathOperator{\argmin}{\text{argmin}}
  \newcommand{\gargmin}[2]{\argmin_{#1}\left\{{#2}\right\} }
  \DeclareMathOperator{\prox}{\mathrm{prox}}
  \newcommand{\proxg}[3]{\prox_{\frac{#1}{#2}{#3}}}
}

\makeatletter
\newcommand*{\transpose}{%
  {\mathpalette\@transpose{}}%
}
\newcommand*{\@transpose}[2]{%
  % #1: math style
  % #2: unused
  \raisebox{0.2em}{$\m@th#1\intercal$}%
}
\makeatother

\newcommand{\tr}[1]{ {#1}^{\! \transpose}}

\xpatchbibmacro{textcite}{\addcoma}{}{}{}

\addbibresource{references.bib}

\usepackage{array,multirow}
\addto\captionsfrench{\def\tablename{TABLEAU}}


\begin{document}

\begin{titlepage}
  \begin{center}

      \makebox[0.5\textwidth][c]{%
        \includegraphics[width=0.33\textwidth]{images/sorbonne.png}%
    }%

    \vspace{4cm}
    % Title
    \HRule \\[0.4cm]
    { \huge \bfseries REDS - Projet Boson de Higgs\\[0.4cm] }

      \textsc{\LARGE Rapport}\\[0.4cm]

    \HRule \\[0.4cm]

    % Author and supervisor
    \begin{minipage}{0.4\textwidth}
      \begin{flushleft} \large
        Kim-Anh Laura \textsc{Nguyen}\\
        Keyvan \textsc{Beroukhim} \\
        Master 2 DAC \\
        Promo 2019-2020 \\
      \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
      \begin{flushright} \large
          \emph{Enseignant :} Olivier \textsc{Schwander} \\
      \end{flushright}
    \end{minipage}

      \vspace{2cm}


  \end{center}
  %\end{sffamily}
\end{titlepage}
%\maketitle

\newpage

\section{Introduction}

Nous souhaitons détecter la présence du boson de Higgs dans des données simulées
dans le but de reproduire le comportement de l'expérience ATLAS. Il s'agit d'un
problème de détection d'évènement, ou classification binaire, dans lequel les
deux classes sont :

\begin{itemize}
    \item \emph{background}
    \item \emph{tau tau decay of a Higgs boson}
\end{itemize}

Afin de répondre à cette problématique, nous commencerons d'abord par analyser
les données afin de les transformer de manière judicieuse. Nous choisirons
ensuite plusieurs classifieurs, dont les hyperparamètres seront optimisés pour
la tâche, et nous les entraînerons sur nos données. Enfin, nous évaluerons ces
méthodes à partir des résultats obtenus. 

\section{Analyse préliminaire des données}

Les données sont issues du projet Kaggle \emph{ATLAS Higgs Boson Machine
Learning Challenge 2014}. Cette base est constituée de 818238 évènements
simulés. Chaque évènement est défini par 30 attributs numériques et un label à
prédire. \\

Les données sont composées à \textbf{34\% de labels positifs} (les
\emph{signaux}) et à 66\% de labels négatifs (le \emph{background}). Les classes
ne sont pas suffisamment déséquilibrées pour gêner l'entraînement. Par ailleurs,
la métrique propre à cette tâche, nommée \emph{AMS}, nous est imposée. Le choix
habituel de la mesure d'évaluation à utiliser pour prendre en compte ce
déséquilibre ne se pose donc pas. \\

\subsection{Distribution des variables}

La \figref{img:hist-sample} contient les histogrammes de répartition des valeurs
de quelques variables, en omettant les valeurs manquantes. Nous constatons que
\textbf{toutes les variables ne suivent pas le même type de distribution}. Or, les
algorithmes d'apprentissage se comportent généralement mieux avec une
distribution des données équilibrée. Dans ce dataset, de nombreuses variables
ont une distribution exponentielle décroissante (e.g \texttt{DER\_MASS\_VIS} sur
la \figref{img:hist-sample}).

\begin{figure}[H]
    \center
    \includegraphics[width=0.5\textwidth]{images/histograms_sample.png}
    \caption{Histogrammes de répartition des valeurs de quelques variables}
    \label{img:hist-sample}
\end{figure}

Afin d’obtenir des \textbf{distributions normales} et de \textbf{stabiliser la
variance}, nous effectuons une \textbf{log-transformation} de manière
indépendante sur les colonnes. Nous commençons par ajouter une constante à
chaque colonne à transformer pour que la valeur minimale de sur cette colonne
soit 1 (comme le log ne prend que des valeurs strictement positives). Nous
appliquons ensuite la fonction log sur les colonnes. Avec ce dataset et en
utilisant cette méthode, nous obtenons toujours des distribution normales.
Finalement nous \textbf{centrons et réduisons} chaque variable de manière
indépendante car cela facilite généralement l’apprentissage des modèles. \\

\begin{figure}[H]
	\centering
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{images/histograms_before_log.png}
    \caption{Avant log-transformation}
    \end{subfigure}

    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{images/histograms_after_log.png}
    \caption{Après log-transformation}
    \end{subfigure}

    \caption{Histogrammes de répartition des valeurs de quelques variables avant
    et après log-transformation}
    \label{fig:hist-log}
\end{figure}

\subsection{Valeurs manquantes}

Le dataset contient \textbf{21\% de valeurs manquantes} (indiquées par la valeur
-999).  D’une manière générale, les modèles d’apprentissage statistique ne
gèrent pas automatiquement les valeurs manquantes. Il faut donc traiter ces
données avant de les présenter à nos modèles. Nous considérons trois façons de
pallier à ce problème.

\subsubsection{Omission}

En supprimant les évènements dont au moins un attribut n'est pas valide, nous
retrouvons avec seulement 223574 exemples d’apprentissage : \textbf{restreindre
le dataset aux données complètes fait perdre 75\% des évènements}. Par ailleurs,
la base restreinte contient 47\% de labels positifs, soit 13\% de plus que le
dataset original. Cela met en évidence le fait que les données ne sont pas
manquantes de manière indépendante : l’absence des données est liée à leur
valeur (nous sommes dans un contexte \emph{Missing Not At Random}). Un modèle
entraîné sur le dataset restreint en faisant l’hypothèse "i.i.d." serait donc
biaisé.

\subsubsection{Omission d'attributs}

Une autre méthode consiste à \textbf{supprimer les features dont le pourcentage
de valeurs manquantes dépasse un certain seuil}. On \textbf{retire ensuite les
évènements à valeurs manquantes}. Le dataset final ne contient donc plus aucune
donnée manquante. En fixant le seuil à 40\%, notre jeu de données passe de
818238 à 693636 échantillons, soit 85\% des données initiales, et de 35 à 20
attributs.


\subsubsection{Conservation}

Les \textbf{\emph{Random Forest}} font partie des algorithmes permettant de
\textbf{gérer les données manquantes}. En effet, les arbres de décision peuvent
\textbf{reconnaître une donnée manquante par un simple test de valeur}.

\subsubsection{Affectation (\emph{Imputing})}

Une autre méthode consiste à \textbf{compléter les données manquantes} de la
manière la plus "intelligente" possible. Une fois les données complétées, cela
permet d’\textbf{utiliser tous les modèles de classification à notre
disposition}. 

Nous nous servons de l’algorithme \texttt{IterativeImputing} de
scikit-learn, qui complète de manière itérative les données manquantes en
utilisant des modèles de régression prédisant la valeur d’une variable à partir
de la valeur des autres variables. \\

En pratique et comme nous souhaitons conserver le maximum d'information,
\textbf{nous ne supprimerons ni d'évènements ni d'attributs à valeurs
manquantes}. Nous testerons, d'un côté, des \textbf{algorithmes permettant de
gérer les données manquantes} et, de l'autre, la méthode de \textbf{complétion
des données}. Pour cette dernière, nous appliquons d'abord les
\textbf{log-transformations} puis nous réalisons l’\textbf{imputing} avant de
finir par \textbf{scaler} les données. Ce choix se justifie premièrement par
l’intuition que l’imputing fonctionnera mieux sur des données log-transformées
et, deuxièmement, par le fait qu’estimer la moyenne et la variance en ignorant
les données manquantes serait biaisé.

\subsection{Sélection de features}

Les données sont représentées par \textbf{30 variables explicatives}. Ce nombre
n’étant \textbf{pas très élevé} (par exemple comparé au nombre de pixels dans
une image), il n’y a donc \textbf{pas de grand risque de sur-apprentissage} pour
les modèles. Ainsi, la sélection de caractéristique ne semble pas nécessaire.
Cependant, comme le montre la \figref{img:corr-mat}, certaines variables sont
\textbf{fortement corrélées} (e.g. \texttt{DER\_mass\_MMC} et
\texttt{DER\_mass\_transverse\_met\_lep}). Or, des attributs corrélés peuvent
diminuer la performance de certains algorithmes (en contribuant au
sur-apprentissage par exemple).

Afin de réduire le nombre de variables explicatives, nous comparerons donc les
performances avec ou sans l’utilisation d’une \textbf{\emph{Analyse en
Composantes Principales} (PCA)}. Pour \textbf{choisir le nombre de composantes
principales}, nous traçons le \textbf{pourcentage de variance expliquée en
fonction du nombre de composantes} (\figref{img:explained-var}). Avec \textbf{20
composantes}, nous conservons $98.2\%$ de la variance totale.

\begin{figure}[H]
    \center
    \includegraphics[width=0.7\textwidth]{images/correlation_matrix.png}
    \caption{Matrice de corrélation entre chaque attribut}
    \label{img:corr-mat}
\end{figure}

\begin{figure}[H]
    \center
    \includegraphics[width=0.75\textwidth]{images/explained_variance.png}
    \caption{Pourcentage de variance expliquée en fonction du nombre de
    composantes}
    \label{img:explained-var}
\end{figure}

\section{Classification des évènements}

\subsection{Modèles utilisés}

Nous considérons plusieurs modèles d'apprentissage pour classifier les
évènements :

\begin{itemize}
    \item SVM
    \item méthodes ensemblistes : Bagging de Perceptron, Random Forest, AdaBoost
\end{itemize}

\subsubsection{Support Vector Machines}

Les \emph{Support Vector Machines} (SVMs) nous fournissent une
\textbf{\emph{baseline}}. Généralisation des classifieurs linéaires, ils
nécessitent un \textbf{faible nombre d'hyperpararamètres}, ont des
\textbf{garanties théoriques} et donnent de \textbf{bons résultats} en pratique.

\subsubsection{Méthodes d'ensemble}

Afin de \textbf{réduire de réduire le biais et la variance} de nos modèles et
ainsi améliorer les performances, nous utilisons des \textbf{méthodes
d'ensemble}. Ces algorithmes consistent à \textbf{combiner plusieurs classifieurs
faibles} pour obtenir un modèle \textbf{plus stable, plus robuste et ayant une meilleure
capacité de généralisation}. \\

\begin{itemize}
    \item Bootstrap AGGregatING (Bagging) : 
        
        Le \emph{Bootstrap Aggregating} est une méthode pour \textbf{réduire la
        variance par moyennage}.  Plusieurs ensembles d'apprentissage sont
        simulés par \textbf{\emph{bootstrap} (tirage avec remise)} puis
        \textbf{sur chaque sous ensemble, on entraîne un classifieur faible}.
        Dans notre contexte de classification binaire, la prédiction du modèle
        est définie par le \textbf{vote majoritaire des modèles simples
        (\emph{aggregating})} .

Comme chaque classifieur faible est entraîné sur des données légèrement
différentes, le modèle d'ensemble est capable de \textbf{capturer de petites
variations dans les données} et donc de mieux \textbf{généraliser}. 

Nous choisissons, uniquement pour le Bagging, le \textbf{Perceptron} comme
classifieur faible. Il s'agit d'un \textbf{modèle instable}, ce qui permet
d'obtenir un \textbf{ensemble de classifieurs suffisamment différents}. \\

\item Random Forest : 
    
    Avec la technique du Bagging, même si chaque classifieur est appris sur un
        jeu de données légèrement différent, tous les modèles se servent des
        mêmes attributs pour partitionner les données, les arbres générés
        risquent donc d'être \textbf{trop similaires}. Pour pallier à ce
        problème, les arbres des \textbf{forêts aléatoires (\emph{random
        forests})}, également appris sur des bootstrap de l'ensemble original,
        \textbf{sélectionnent pour chacun de leurs noeuds un échantillon
        aléatoire de features}, ce qui \textbf{force les modèles à être
        différents} et empêche le modèle global de se concentrer sur des
        attributs particuliers. \\

    \item AdaBoost : 
        
        Avec la technique de Bagging, les classifieurs faibles sont appris
        indépendamment les uns des autres. Or, si ce classifieur est, à lui
        seul, trop faible, \textbf{le Bagging ne permettra pas d'obtenir un
        meilleur biais}. Le \textbf{\emph{Boosting}} permet de traiter ce
        problème par \textbf{apprentissage successif de classifieurs faibles} :
        en donnant plus de poids aux exemples mal classés par les modèles
        précédents, \textbf{chaque nouveau classifieur se focalise sur les
        parties de l'espace mal prédites}.  Contrairement au Bagging, la
        contribution d'un classifieur à la prédiction du modèle d'ensemble est
        déterminée par sa performance.

En revanche, si le classifieur faible est trop complexe, le modèle de Boosting
        \textbf{risque de sur-apprendre}, auquel cas la technique de Bagging est
        plus adaptée.

\end{itemize}

\subsection{Protocole d'expérimentation} 

Tout d'abord, nous commençons par \textbf{séparer le jeu de données en train et
en test}. Le protocole d'expérimentation est le suivant :

\begin{enumerate}[label=\alph*.]
    
    \item   Avant d'entraîner nos modèles, nous apprenons les
        \textbf{paramètres de prétraitement des données} sur l'ensemble
        d'apprentissage, et nous appliquons les prétraitements sur l'ensemble
        des données.  Les prétraitements et apprentissages des paramètres sont
        effectués dans l'ordre décrit précédemment: \textbf{log-transformation}
        (valeurs minimales des colonnes apprises), \textbf{imputing} (paramètres
        de tous les régresseurs appris) et \textbf{scaling} (moyennes et
        écarts-types des colonnes appris). 

\item Pour chaque algorithme, nous procédons par \textbf{\emph{grid search} pour
    trouver les hyperparamètres optimaux}. Pour chaque combinaison
        d'hyperparamètres, la performance du modèle produit est estimée par
        \textbf{\emph{cross-validation}} sur l'ensemble d'apprentissage.

\item Pour chaque modèle, nous récupérons les hyperparamètres ayant produit le
    meilleur résultat et \textbf{calculons le score final par cross-validation
        sur les données de test}. \\

\end{enumerate}

Nous comparons les résultats obtenus à ceux produits en appliquant aux données,
\textbf{en plus des prétraitements de l'étape a.}, une \textbf{PCA} (à 20
composantes principales). \\

En parallèle, pour pouvoir \textbf{comparer l'approche de complétion à celle de
conservation des données}, nous menons d'autres expériences où les données sont
prétraitées de la manière suivante : log-transformation, scaling et
\textbf{remplacement des valeurs manquantes par une valeur aberrante}. Nous ne
rajoutons pas de PCA à ces prétraitements. 

\textbf{Le seul modèle entraîné sur le jeu de données contenant des données
aberrantes} est un \textbf{Random Forest} que nous nommerons \textbf{RFnan}. \\

Par souci de temps de calcul, les expériences de recherche de paramètres
optimaux et d'évaluation des modèles ne sont pas menées sur le dataset entier.
Pour les \textbf{métriques usuelles}, \textbf{le score obtenu par un modèle est
indépendant de la taille du jeu de test}, mais \textbf{ce n'est pas le cas pour
la métrique AMS}.  Afin d'obtenir des scores en AMS comparables, nous fixons la
taille des jeux de tests à 1000 éléments. \\

\textbf{Les scores obtenus en AMS étant difficilement interprétables}, nous
utilisons aussi comme métrique le \textbf{taux de bonne classification
(\emph{accuracy})}. Pour chaque métrique, les scores présentés par la suite sont
obtenus en optimisant les modèles pour cette métrique.

\subsection{Résultats et analyse}

Pour le \textbf{SVM}, nous cherchons la valeur optimale de la \textbf{pénalité
$\bm{C}$ du terme d'erreur} ainsi que le meilleur \textbf{noyau} à utiliser. 

$C$ correspond à la \textbf{constante de \emph{soft margin}}. Cet hyperparamètre
influe sur le \textbf{compromis entre l'erreur en apprentissage et la
maximisation de la marge}.  Plus sa valeur est petite, plus le modèle sera
tolérant envers les exemples mal classés ou dans la marge, ce qui correspond à
élargir cette dernière. Nous faisons varier $C$ parmi \textbf{100 valeurs entre
0.1 et 10} équitablement réparties sur une échelle logarithmique.

De plus, nous cherchons le noyau le plus adapté : \textbf{\emph{Radial Basis
Function} (rbf) ou un noyau polynomial (poly)}. Cet hyperparamètre définit le
\textbf{type d'hyperplan} utilisé pour séparer les données.  \\

La \figref{tab:svm-parameters} contient les \textbf{hyperpamètres optimisant le
SVM pour chaque métrique, avec ou sans PCA}. Nous constatons que le
\textbf{noyau rbf} permet d'obtenir les meilleures performances dans chaque cas,
et que les \textbf{valeurs de $\bm{C}$ maximisant les scores en AMS sont similaires à celles
maximisant l'accuracy}.\\

\begin{table}[H]
    \centering
    \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{c|*{3}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{Accuracy} &
            \multicolumn{2}{c}{AMS}
            \\ \cline{2-3} \cline{4-5}
            & sans PCA & avec PCA & sans PCA & avec PCA\\
            \hline
            C & 0.98 & 0.74 & 0.89 & 1.71 \\
            Noyau & rbf & rbf & rbf & rbf \\
        \end{tabular}}
    \caption{Paramètres optimaux pour le SVM}
    \label{tab:svm-parameters}
\end{table}

Pour les \textbf{méthodes d'ensemble,} nous faisons varier le \textbf{nombre
d'estimateurs} (50, 500, 1000 ou 2000), qui correspond à
\textbf{l'hyperparamètre influant le plus sur la précision de la prédiction}. De
plus, nous cherchons la \textbf{profondeur maximale optimale des Random
Forests}.  \\

\textbf{Les hyperparamètres optimaux obtenus pour chaque méthode d'ensemble,
chaque métrique et en considérant l'intégralité des variables} figurent dans le
\tabref{tab:ensemble-parameters}. Nous ne remarquons \textbf{pas de phénomènes
particuliers} : ces hyperparamètres n'évoluent pas de façon monotone lorsque
nous passons d'une méthode/métrique à une autre.  Dans le cas des données
réduites à 20 composantes, les hyperparamètres optimaux obtenus sont, pour la
plupart, différents des précédents et, une fois de plus, nous ne constatons pas
d'évolution particulière. Nous décidons donc de ne pas les afficher. \\

\begin{table}[H]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|*{7}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{Bagging} &
            \multicolumn{2}{c|}{RF} &
            \multicolumn{2}{c|}{RFNan} &
            \multicolumn{2}{c}{AdaBoost}
            \\ \cline{2-3} \cline{4-5} \cline{6-7} \cline{8-9}
            & Accuracy & AMS & Accuracy & AMS & Accuracy & AMS & Accuracy & AMS\\
            \hline
            Nombre d'estimateurs & 500 & 500 & 500 & 500 & 1000 & 2000 & 1000 & 50 \\
            Profondeur max & \cellcolor{gray!40} & \cellcolor{gray!40} & $\infty$ & 20 &
            50 & $\infty$ & \cellcolor{gray!40} & \cellcolor{gray!40} \\
        \end{tabular}}
    \caption{Paramètres optimaux pour les méthodes d'ensemble (sans PCA)}
    \label{tab:ensemble-parameters}
\end{table}

%\begin{table}[H]
%    \resizebox{\textwidth}{!}{
%        \begin{tabular}{c|*{5}{c|}c}
%            \multirow{2}{*}{} &
%            \multicolumn{2}{c|}{Bagging} &
%            \multicolumn{2}{c|}{RF} &
%            \multicolumn{2}{c}{AdaBoost}
%            \\ \cline{2-3} \cline{4-5} \cline{6-7} 
%            & Accuracy & AMS & Accuracy & AMS & Accuracy & AMS\\
%            \hline
%            Nombre d'estimateurs & 2000 & 500 & 500 & 2000 & 500 & 50 \\
%            Profondeur max & \cellcolor{gray!40} & \cellcolor{gray!40} & 50 & 50 & \cellcolor{gray!40} & \cellcolor{gray!40} \\
%        \end{tabular}}
%    \caption{Paramètres optimaux pour les méthodes d'ensemble (avec PCA)}
%    \label{tab:ensemble-parameters-pca}
%\end{table}

Sur la \figref{img:ams-C} est tracé \textbf{l'AMS moyen obtenu en apprentissage
en fonction de la valeur de $\bm{C}$ (sans PCA)}. Nous remarquons que le score
augmente jusqu'à $C \approx 4$ puis stagne.

\begin{figure}[H]
    \center
    \includegraphics[width=0.7\textwidth]{images/ams.png}
    \caption{AMS moyen obtenu avec le SVM en fonction de la valeur de $C$}
    \label{img:ams-C}
\end{figure}

Les tableaux \ref{tab:ensemble-results} et \ref{tab:ensemble-results-pca}
contiennent les \textbf{scores en accuracy et AMS obtenus avec chaque méthode
et, respectivement, sans et avec PCA}. Pour chaque métrique et chaque ensemble
sur lequel la performance est calculée, le meilleur score est surligné en vert.

\begin{table}[H]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|*{3}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{\% Accuracy} &
            \multicolumn{2}{c}{AMS ($10^{-2}$)}\\ \cline{2-3} \cline{4-5}
            & Apprentissage & Test & Apprentissage & Test \\
            \hline
            SVM & $75.90 \pm 1.67$ & $75.00 \pm 1.16$ & $ 4.35 \pm 0.39 $ &
            $3.19 \pm 0.52$ \\
            Bagging & $71.90 \pm 1.29$ & $72.60 \pm 0.98$ & $3.16 \pm 0.60$ &
            $2.23 \pm 0.24$\\
            RF & \cellcolor{green!25}$80.70 \pm 1.24$ & $78.70 \pm 1.59$ & $5.07
            \pm 0.96$ & $3.98 \pm 0.16$\\
            RFNan & $80.20 \pm 0.44$ & \cellcolor{green!25}$81.30 \pm 0.62 $ &
            \cellcolor{green!25}$5.15 \pm 1.12$ & \cellcolor{green!25}$4.00 \pm
            0.15$\\
            AdaBoost & $73.30 \pm 0.76$ & $73.30 \pm 1.49$ & $3.92 \pm 0.55$ &
            $3.42 \pm 0.16$\\
        \end{tabular}}
    \caption{Performances obtenues avec chaque méthode (sans PCA)}
    \label{tab:ensemble-results}
\end{table}


\begin{table}[H]
    \center
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|*{3}{c|}c}
            \multirow{2}{*}{} &
            \multicolumn{2}{c|}{\% Accuracy} &
            \multicolumn{2}{c}{AMS ($10^{-2}$)}\\ \cline{2-3} \cline{4-5}
            & Apprentissage & Test & Apprentissage & Test \\
            \hline
            SVM & $76.30 \pm 1.49$ & $
            \cellcolor{green!25}75.60 \pm 0.45 $ & \cellcolor{green!25}$4.38 \pm
            0.36$ & \cellcolor{green!25}$3.02 \pm 0.12 $\\
            Bagging & $71.70 \pm 1.93$ & $70.30 \pm 0.95$ & $2.20 \pm 0.49$ &
            $1.91 \pm 0.06$  \\ 
            RF & \cellcolor{green!25}$77.20 \pm 1.48$ & $74.60 \pm 1.37 $ &
            $3.78 \pm 0.56$ & $2.69
            \pm 0.17$\\
            AdaBoost & $68.00 \pm 1.30$ & $68.00 \pm  0.99$ & $3.04 \pm 0.26$ &
            $2.05 \pm 0.26$ \\
        \end{tabular}}
    \caption{Performances obtenues avec chaque méthode (avec PCA)}
    \label{tab:ensemble-results-pca}
\end{table}

Les performances obtenues avec PCA sont toujours inférieures à celles obtenues
sans. \textbf{L'utilisation d'une PCA n'est donc pas judicieuse.}

%\begin{figure}[H]
%    \center
%    \includegraphics[width=0.7\textwidth]{images/learning_curve.png}
%    \caption{Précisions obtenues en fonction de la taille d'apprentissage (échelle logarithmique)}
%    \label{img:ams-C}
%\end{figure}
%\todo[inline, color=yellow!40]{Remplacer par les nouveaux plots \\}

\textbf{Les meilleurs modèles sont RF et RFnan}, leur différence de score n'est
pas suffisante pour pouvoir les départager.  En relançant les expériences sur 10
fois plus de données (le temps d'exécution devient trop important au delà), nous
obtenons les performances AMS (multipliées par $10^{-2}$) suivantes :

\begin{itemize}
    \item RF :     $18.20 \pm 1.39$
    \item RFnan :  $18.29 \pm 1.75$
\end{itemize}

Les scores obtenus sont encore une fois très similaires, \textbf{nous préférerons
cependant le modèle RFnan qui ne nécessite pas d'entraîner des régresseurs pour
compléter les données lors du prétraitement}. 

\begin{figure}[H]
    \center
    \includegraphics[width=\textwidth]{images/ROC_curves.png}
    \caption{Courbes ROC pour chaque méthode}
    \label{img:roc-curves}
\end{figure}

La \figref{img:roc-curves} contient les \textbf{courbes ROC} (i.e. le taux de
vrais positifs en fonction du taux de faux positifs) \textbf{obtenues avec
chaque méthode} ainsi qu'avec un classifieur aléatoire. Nous constatons que les
courbes correspondant au \textbf{SVM et à AdaBoost} sont \textbf{plus éloignées
du coude du classifieur idéal} (qui passe de $(0,0)$ à $(0,1)$ à $(1,1)$) que
les autres. 

Afin d'avoir une mesure de qualité d'un modèle indépendante des seuils de
classification, nous calculons également \textbf{l'aire sous la courbe ROC (AUC)
correspondant à chaque méthode} (\tabref{tab:AUC-scores}). Les modèles
\textbf{RF et RFNan obtiennent les meilleurs scores} : ce sont ceux qui
distinguent le mieux les deux classes, i.e. ils sont meilleurs que les autres
pour étiqueter le background comme étant du background, et le signal comme étant
du signal. \\

\begin{table}[H]
\centering
\resizebox{0.25\textwidth}{!}{
\begin{tabular}{c|c}
         & \% AUC\\
        \hline
        SVM & 81.95\\
        Bagging & 85.5\\
        RF & \cellcolor{green!25} 86.20\\
        RFNan & 85.88\\
        AdaBoost & 80.45\\
\end{tabular}}
\caption{AUC obtenus avec chaque méthode}
\label{tab:AUC-scores}
\end{table}

\section{Conclusion}

Ce projet nous a permis de mettre en place une chaîne de traitement des données
standard pour une tâche de classification binaire. Nous avons prétraité les
données de sorte à ré-équilibrer les distributions de variables et pallier au
problème des données manquantes. Une baseline ainsi que plusieurs méthodes
d'ensemble ont ensuite été mis en place pour répondre au problème de détection
du boson de Higgs. Enfin, nous avons évalué et comparé entre eux chaque modèle
selon plusieurs métriques.

\todo[inline, color=yellow!40]{Difficultés et perspectives}

\newpage
\printbibliography
\end{document}

